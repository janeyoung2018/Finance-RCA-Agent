# Copy to .env and fill in values to enable optional LLM summaries.

# Required for LLM decision-support (omit to stay rule-based)
# Prefer Gemini (free tier friendly)
# GOOGLE_API_KEY=your-gemini-api-key
# LLM_MODEL=gemini-1.5-flash

# Optional overrides
# OPENAI_API_KEY=sk-...
# OPENAI_BASE_URL=https://api.openai.com/v1
# LLM_MAX_TOKENS=256
# LLM_TEMPERATURE=0.2

# Observability / Phoenix
# OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:6006/v1
# SERVICE_VERSION=0.1.0
# APP_ENV=dev
# LLM_PROMPT_COST_PER_1K=0.0
# LLM_COMPLETION_COST_PER_1K=0.0

# Storage
# RUN_STORE_PATH=/custom/path/run_store.sqlite

# Security & hardening
# API_KEY=change-me
# RATE_LIMIT_REQUESTS=60
# RATE_LIMIT_WINDOW_SECONDS=60
# MAX_CONCURRENT_RUNS=2
# MAX_QUEUED_RUNS=10
